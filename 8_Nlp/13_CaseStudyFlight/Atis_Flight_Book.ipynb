{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libaries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk, pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import gzip, os, pickle # gzip for reading the gz files, pickle to save/dump trained model \n",
    "import _pickle as cPickle\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'atis.fold0.pkl.gz'\n",
    "f = gzip.open(filename, 'rb')\n",
    "try:\n",
    "    train_set, valid_set, test_set, dicts = pickle.load(f, encoding='latin1')\n",
    "except:\n",
    "    train_set, valid_set, test_set, dicts = pickle.load(f)\n",
    "finally:\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "\n",
      "<class 'list'> <class 'list'> <class 'list'>\n",
      "3983 3983 3983\n"
     ]
    }
   ],
   "source": [
    "print(type(train_set))\n",
    "print()\n",
    "\n",
    "# types of the three elements in the tuple\n",
    "print(type(train_set[0]), type(train_set[1]), type(train_set[2]))\n",
    "print(len(train_set[0]), len(train_set[1]), len(train_set[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'list'> <class 'list'>\n",
      "995 995 995\n",
      "\n",
      "<class 'list'> <class 'list'> <class 'list'>\n",
      "893 893 893\n"
     ]
    }
   ],
   "source": [
    "print(type(valid_set[0]), type(valid_set[1]), type(valid_set[2]))\n",
    "print(len(valid_set[0]), len(valid_set[1]), len(valid_set[2]))\n",
    "print()\n",
    "\n",
    "# test set\n",
    "print(type(test_set[0]), type(test_set[1]), type(test_set[2]))\n",
    "print(len(test_set[0]), len(test_set[1]), len(test_set[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([554, 194, 268,  64,  62,  16,   8, 234, 481,  20,  40,  58, 234,\n",
      "       415, 205]),\n",
      " array([554, 241, 481,  14, 200,  91,  26, 239]),\n",
      " array([232,   0, 273, 502, 254, 481, 165, 193, 208,  77, 502,  64])]\n",
      "##################################################\n",
      "[array([  0,   0,   0,  18,   0,   1,  52,   0,   0,  76,   0,   0,   0,\n",
      "        18, 109]),\n",
      " array([  0,   0,   0,   0,   0,   6, 107, 107]),\n",
      " array([ 0,  0,  0,  0,  0,  0, 44,  0,  0, 18,  0, 18])]\n",
      "##################################################\n",
      "[array([126, 126, 126,  48, 126,  36,  35, 126, 126,  33, 126, 126, 126,\n",
      "        78, 123]),\n",
      " array([126, 126, 126, 126, 126,   2,  83,  83]),\n",
      " array([126, 126, 126, 126, 126, 126,  42, 126, 126,  48, 126,  78])]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(train_set[0][:3])\n",
    "print('#'*50)\n",
    "pprint.pprint(train_set[1][:3])\n",
    "print('#'*50)\n",
    "pprint.pprint(train_set[2][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, _, train_label = train_set\n",
    "val_x, _, val_label = valid_set\n",
    "test_x, _, test_label = test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([554, 194, 268,  64,  62,  16,   8, 234, 481,  20,  40,  58, 234,\n",
       "       415, 205])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([126, 126, 126,  48, 126,  36,  35, 126, 126,  33, 126, 126, 126,\n",
       "        78, 123])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['labels2idx', 'tables2idx', 'words2idx'])\n"
     ]
    }
   ],
   "source": [
    "print(type(dicts))\n",
    "print(dicts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(dicts['labels2idx']))\n",
    "print(type(dicts['tables2idx']))\n",
    "print(type(dicts['words2idx']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = dicts['words2idx']\n",
    "labels = dicts['labels2idx']\n",
    "tables = dicts['tables2idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('detroit', 145),\n",
       " ('beach', 71),\n",
       " ('friends', 207),\n",
       " ('business', 82),\n",
       " ('air', 22),\n",
       " ('mitchell', 316),\n",
       " ('connections', 119),\n",
       " ('traveling', 514),\n",
       " ('so', 446),\n",
       " ('kind', 252)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(words.items(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 'flights',\n",
       " 'leave',\n",
       " 'atlanta',\n",
       " 'at',\n",
       " 'about',\n",
       " 'DIGIT',\n",
       " 'in',\n",
       " 'the',\n",
       " 'afternoon',\n",
       " 'and',\n",
       " 'arrive',\n",
       " 'in',\n",
       " 'san',\n",
       " 'francisco']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for val in train_x[0] for k,v in words.items() if v==val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what flights leave atlanta at about DIGIT in the afternoon and arrive in san francisco',\n",
       " 'what is the abbreviation for canadian airlines international',\n",
       " \"i 'd like to know the earliest flight from boston to atlanta\",\n",
       " 'show me the us air flights from atlanta to boston',\n",
       " 'show me the cheapest round trips from dallas to baltimore',\n",
       " \"i 'd like to see all flights from denver to philadelphia\",\n",
       " 'explain fare code qx',\n",
       " \"i 'd like a united airlines flight on wednesday from san francisco to boston\",\n",
       " 'what is the price of american airlines flight DIGITDIGIT from new york to los angeles',\n",
       " 'what does the meal code s stand for',\n",
       " 'what are all flights to denver from philadelphia on sunday',\n",
       " 'what times does the late afternoon flight leave from washington for denver',\n",
       " 'what flights are available monday from san francisco to pittsburgh',\n",
       " 'what airlines have business class',\n",
       " 'flights from atlanta to washington dc',\n",
       " 'from new york to toronto on thursday morning',\n",
       " 'show me all the direct flights from atlanta to baltimore',\n",
       " 'list the flights from new york to miami on a tuesday which are nonstop and cost less than DIGITDIGITDIGIT dollars',\n",
       " 'show me the first flight that arrives in toronto from cincinnati',\n",
       " 'what planes are used by twa',\n",
       " 'please give me the prices for all flights from philadelphia to denver airport next sunday',\n",
       " 'show me all flights from pittsburgh to oakland that arrive after DIGITDIGIT am',\n",
       " 'what is the least expensive flight today from atlanta to san francisco',\n",
       " 'i want a flight from philadelphia to dallas with a stop in atlanta',\n",
       " 'show me the flights from baltimore to philadelphia',\n",
       " 'what airlines fly from st. petersburg to milwaukee and from milwaukee to tacoma',\n",
       " 'please give me the flights from san francisco to washington dc',\n",
       " 'i need a flight delta airlines kansas city to salt lake',\n",
       " 'show me flights going from boston to denver arriving on wednesday morning',\n",
       " 'show me flights leaving from denver colorado to pittsburgh pennsylvania on wednesdays after DIGIT pm']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = []\n",
    "for i in range(30):\n",
    "    sents.append(' '.join([k for val in train_x[i] for k,v in words.items() if v==val]))\n",
    "\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B-depart_date.day_name', 26),\n",
       " ('B-depart_date.date_relative', 25),\n",
       " ('B-toloc.city_name', 78),\n",
       " ('B-toloc.state_code', 80),\n",
       " ('B-meal_description', 53),\n",
       " ('I-round_trip', 117),\n",
       " ('I-arrive_time.start_time', 88),\n",
       " ('B-fare_basis_code', 39),\n",
       " ('I-toloc.airport_name', 122),\n",
       " ('B-depart_date.today_relative', 29),\n",
       " ('B-depart_time.time_relative', 36),\n",
       " ('B-depart_time.time', 35),\n",
       " ('B-depart_date.year', 30),\n",
       " ('I-toloc.state_name', 124),\n",
       " ('I-fromloc.city_name', 109),\n",
       " ('B-state_name', 68),\n",
       " ('B-state_code', 67),\n",
       " ('B-return_date.month_name', 62),\n",
       " ('I-today_relative', 121),\n",
       " ('I-flight_mod', 104),\n",
       " ('I-depart_time.period_of_day', 97),\n",
       " ('B-toloc.country_name', 79),\n",
       " ('B-cost_relative', 21),\n",
       " ('B-mod', 54),\n",
       " ('B-round_trip', 66)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(labels.items(), 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127\n"
     ]
    }
   ],
   "source": [
    "print(len(labels.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_words = {words[k]:k for k in words}\n",
    "id_to_labels = {labels[k]:k for k in labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_query(index):\n",
    "    w = [id_to_words[id] for id in train_x[index]]\n",
    "    l = [id_to_labels[id] for id in train_label[index]]\n",
    "    return list(zip(w, l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('please', 'O'),\n",
       " ('show', 'O'),\n",
       " ('me', 'O'),\n",
       " ('the', 'O'),\n",
       " ('return', 'O'),\n",
       " ('flight', 'O'),\n",
       " ('number', 'O'),\n",
       " ('from', 'O'),\n",
       " ('toronto', 'B-fromloc.city_name'),\n",
       " ('to', 'O'),\n",
       " ('st.', 'B-toloc.city_name'),\n",
       " ('petersburg', 'I-toloc.city_name')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_query(3900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 'O'),\n",
       " ('there', 'O'),\n",
       " ('a', 'O'),\n",
       " ('flight', 'O'),\n",
       " ('between', 'O'),\n",
       " ('oakland', 'B-fromloc.city_name'),\n",
       " ('and', 'O'),\n",
       " ('boston', 'B-toloc.city_name'),\n",
       " ('with', 'O'),\n",
       " ('a', 'O'),\n",
       " ('stopover', 'O'),\n",
       " ('in', 'O'),\n",
       " ('dallas', 'B-stoploc.city_name'),\n",
       " ('fort', 'I-stoploc.city_name'),\n",
       " ('worth', 'I-stoploc.city_name'),\n",
       " ('on', 'O'),\n",
       " ('twa', 'B-airline_code')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_query(3443)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('which', 'O'),\n",
       " ('airline', 'O'),\n",
       " ('has', 'O'),\n",
       " ('the', 'O'),\n",
       " ('smallest', 'B-mod'),\n",
       " ('plane', 'O'),\n",
       " ('leaving', 'O'),\n",
       " ('pittsburgh', 'B-fromloc.city_name'),\n",
       " ('and', 'O'),\n",
       " ('arriving', 'O'),\n",
       " ('in', 'O'),\n",
       " ('baltimore', 'B-toloc.city_name'),\n",
       " ('on', 'O'),\n",
       " ('july', 'B-arrive_date.month_name'),\n",
       " ('fourth', 'B-arrive_date.day_number')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=random.randrange(len(train_x))\n",
    "print_query(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'O'),\n",
       " ('need', 'O'),\n",
       " ('information', 'O'),\n",
       " ('on', 'O'),\n",
       " ('a', 'O'),\n",
       " ('flight', 'O'),\n",
       " ('from', 'O'),\n",
       " ('washington', 'B-fromloc.city_name'),\n",
       " ('to', 'O'),\n",
       " ('fort', 'B-toloc.city_name'),\n",
       " ('worth', 'I-toloc.city_name')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=random.randrange(len(train_x))\n",
    "print_query(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\avinash.tiwari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pos_tag(sent_list):\n",
    "    pos_tags = []    \n",
    "    for sent in sent_list:\n",
    "        tagged_words = nltk.pos_tag([id_to_words[val] for val in sent])\n",
    "        pos_tags.append(tagged_words)\n",
    "    return pos_tags\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = pos_tag(train_x)\n",
    "valid_pos = pos_tag(val_x)\n",
    "test_pos = pos_tag(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('what', 'WP'),\n",
       " ('flights', 'NNS'),\n",
       " ('from', 'IN'),\n",
       " ('new', 'JJ'),\n",
       " ('york', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('los', 'VB'),\n",
       " ('angeles', 'NNS')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = random.randrange(len(train_pos))\n",
    "train_pos[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_pos_label(pos_tagged_data, labels):\n",
    "    iob_labels = []         # initialize the list of 3-tuples to be returned\n",
    "    \n",
    "    for sent in list(zip(pos_tagged_data, labels)):\n",
    "        pos = sent[0]       \n",
    "        labels = sent[1]    \n",
    "        zipped_list = list(zip(pos, labels)) # [(word, pos), label]\n",
    "        \n",
    "        # create (word, pos, label) tuples from zipped list\n",
    "        tuple_3 = [(word_pos_tuple[0], word_pos_tuple[1], id_to_labels[label]) \n",
    "                   for word_pos_tuple, label in zipped_list]\n",
    "        iob_labels.append(tuple_3)\n",
    "    return iob_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('show', 'VB', 'O'),\n",
       "  ('me', 'PRP', 'O'),\n",
       "  ('the', 'DT', 'O'),\n",
       "  ('cheapest', 'JJS', 'B-cost_relative'),\n",
       "  ('round', 'NN', 'B-round_trip'),\n",
       "  ('trips', 'NNS', 'I-round_trip'),\n",
       "  ('from', 'IN', 'O'),\n",
       "  ('dallas', 'NN', 'B-fromloc.city_name'),\n",
       "  ('to', 'TO', 'O'),\n",
       "  ('baltimore', 'VB', 'B-toloc.city_name')],\n",
       " [('i', 'JJ', 'O'),\n",
       "  (\"'d\", 'MD', 'O'),\n",
       "  ('like', 'VB', 'O'),\n",
       "  ('to', 'TO', 'O'),\n",
       "  ('see', 'VB', 'O'),\n",
       "  ('all', 'DT', 'O'),\n",
       "  ('flights', 'NNS', 'O'),\n",
       "  ('from', 'IN', 'O'),\n",
       "  ('denver', 'NN', 'B-fromloc.city_name'),\n",
       "  ('to', 'TO', 'O'),\n",
       "  ('philadelphia', 'VB', 'B-toloc.city_name')]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = create_word_pos_label(train_pos, train_label)\n",
    "train_labels[4:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_labels = create_word_pos_label(valid_pos, val_label)\n",
    "test_labels = create_word_pos_label(test_pos, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('show', 'VB', 'O'),\n",
       " ('me', 'PRP', 'O'),\n",
       " ('the', 'DT', 'O'),\n",
       " ('us', 'PRP', 'B-airline_name'),\n",
       " ('air', 'NN', 'I-airline_name'),\n",
       " ('flights', 'NNS', 'O'),\n",
       " ('from', 'IN', 'O'),\n",
       " ('atlanta', 'NN', 'B-fromloc.city_name'),\n",
       " ('to', 'TO', 'O'),\n",
       " ('boston', 'VB', 'B-toloc.city_name')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import conll2000\n",
    "from nltk import conlltags2tree, tree2conlltags\n",
    "\n",
    "# print a sample tree in tuple format\n",
    "train_labels[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  show/VB\n",
      "  me/PRP\n",
      "  the/DT\n",
      "  (airline_name us/PRP air/NN)\n",
      "  flights/NNS\n",
      "  from/IN\n",
      "  (fromloc.city_name atlanta/NN)\n",
      "  to/TO\n",
      "  (toloc.city_name boston/VB))\n"
     ]
    }
   ],
   "source": [
    "tree = conlltags2tree(train_labels[3])\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trees = [conlltags2tree(sent) for sent in train_labels]\n",
    "valid_trees = [conlltags2tree(sent) for sent in valid_labels]\n",
    "test_trees = [conlltags2tree(sent) for sent in test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  i/NNS\n",
      "  need/VBP\n",
      "  your/PRP$\n",
      "  help/NN\n",
      "  with/IN\n",
      "  information/NN\n",
      "  on/IN\n",
      "  ground/NN\n",
      "  transportation/NN\n",
      "  from/IN\n",
      "  the/DT\n",
      "  airport/NN\n",
      "  in/IN\n",
      "  (fromloc.city_name philadelphia/NN)\n",
      "  to/TO\n",
      "  downtown/VB)\n"
     ]
    }
   ],
   "source": [
    "i=random.randrange(len(train_trees))\n",
    "print(train_trees[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  show/VB\n",
      "  me/PRP\n",
      "  flights/NNS\n",
      "  on/IN\n",
      "  (depart_date.day_name sunday/NN)\n",
      "  going/VBG\n",
      "  from/IN\n",
      "  (fromloc.city_name san/JJ francisco/NN)\n",
      "  to/TO\n",
      "  (toloc.city_name boston/VB)\n",
      "  (flight_stop nonstop/JJ)\n",
      "  (class_type first/JJ class/NN)\n",
      "  leaving/NN\n",
      "  (depart_time.time_relative after/IN)\n",
      "  (depart_time.time DIGITDIGIT/NNP noon/NN))\n"
     ]
    }
   ],
   "source": [
    "print(train_trees[3468])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),\n",
    "            (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"), (\"the\", \"DT\"), (\"cat\", \"NN\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"NP_chunk: {<DT>?<NN><JJ>*}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  the/DT\n",
      "  little/JJ\n",
      "  yellow/JJ\n",
      "  (NP_chunk dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP_chunk the/DT cat/NN))\n"
     ]
    }
   ],
   "source": [
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(sentence)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 'DT', 'O'),\n",
       " ('little', 'JJ', 'O'),\n",
       " ('yellow', 'JJ', 'O'),\n",
       " ('dog', 'NN', 'B-NP_chunk'),\n",
       " ('barked', 'VBD', 'O'),\n",
       " ('at', 'IN', 'O'),\n",
       " ('the', 'DT', 'B-NP_chunk'),\n",
       " ('cat', 'NN', 'I-NP_chunk')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree2conlltags(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  63.8%%\n",
      "    Precision:      0.0%%\n",
      "    Recall:         0.0%%\n",
      "    F-Measure:      0.0%%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "grammar = ''\n",
    "\n",
    "# initialise cp \n",
    "cp = nltk.RegexpParser(grammar)\n",
    "\n",
    "# evaluate results against actual IOB labels\n",
    "result = cp.evaluate(train_trees)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  could/MD\n",
      "  you/PRP\n",
      "  tell/VB\n",
      "  me/PRP\n",
      "  about/IN\n",
      "  flights/NNS\n",
      "  from/IN\n",
      "  (fromloc.city_name philadelphia/NN)\n",
      "  to/TO\n",
      "  (toloc.city_name dallas/VB)\n",
      "  the/DT\n",
      "  flights/NNS\n",
      "  should/MD\n",
      "  leave/VB\n",
      "  (fromloc.city_name philadelphia/NN)\n",
      "  in/IN\n",
      "  the/DT\n",
      "  (depart_time.period_of_day morning/NN))\n"
     ]
    }
   ],
   "source": [
    "print(train_trees[random.randrange(len(train_trees))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  65.2%%\n",
      "    Precision:     31.7%%\n",
      "    Recall:        40.9%%\n",
      "    F-Measure:     35.7%%\n"
     ]
    }
   ],
   "source": [
    "grammar = '''\n",
    "fromloc.city_name: {<JJ>?<NN>}\n",
    "toloc.city_name: {<VB><NN>?}\n",
    "'''\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.evaluate(train_trees)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ChunkParserI\n",
    "\n",
    "class UnigramChunker(ChunkParserI):    \n",
    "    def __init__(self, train_sents):\n",
    "        # convert train sents from tree format to tags\n",
    "        train_data = [[(t, c) for w, t, c in nltk.chunk.tree2conlltags(sent)] \n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.UnigramTagger(train_data)\n",
    "        \n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word, pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        \n",
    "        # convert to tree again\n",
    "        conlltags = [(word, pos, chunktag) for ((word, pos), chunktag) in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  66.3%%\n",
      "    Precision:     37.5%%\n",
      "    Recall:        18.5%%\n",
      "    F-Measure:     24.8%%\n"
     ]
    }
   ],
   "source": [
    "unigram_chunker = UnigramChunker(train_trees)\n",
    "print(unigram_chunker.evaluate(valid_trees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('CC', 'O'), ('CD', 'B-round_trip'), ('DT', 'O'), ('EX', 'O'), ('FW', 'B-fromloc.city_name'), ('IN', 'O'), ('JJ', 'O'), ('JJR', 'B-cost_relative'), ('JJS', 'B-cost_relative'), ('MD', 'O'), ('NN', 'O'), ('NNP', 'B-depart_time.time'), ('NNS', 'O'), ('PDT', 'O'), ('POS', 'O'), ('PRP', 'O'), ('PRP$', 'O'), ('RB', 'O'), ('RBR', 'B-cost_relative'), ('RBS', 'B-cost_relative'), ('RP', 'O'), ('TO', 'O'), ('VB', 'B-toloc.city_name'), ('VBD', 'O'), ('VBG', 'O'), ('VBN', 'O'), ('VBP', 'O'), ('VBZ', 'O'), ('WDT', 'O'), ('WP', 'O'), ('WRB', 'O')]\n"
     ]
    }
   ],
   "source": [
    "postags = sorted(set([pos for sent in train_trees for (word, pos) in sent.leaves()]))\n",
    "\n",
    "# for each tag, assign the most likely IOB label\n",
    "print(unigram_chunker.tagger.tag(postags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BigramChunker(ChunkParserI):    \n",
    "    def __init__(self, train_sents):\n",
    "        # convert train sents from tree format to tags\n",
    "        train_data = [[(t, c) for w, t, c in nltk.chunk.tree2conlltags(sent)] \n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.BigramTagger(train_data)\n",
    "        \n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word, pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        \n",
    "        # convert to tree again\n",
    "        conlltags = [(word, pos, chunktag) for ((word, pos), chunktag) in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  70.6%%\n",
      "    Precision:     43.5%%\n",
      "    Recall:        38.8%%\n",
      "    F-Measure:     41.0%%\n"
     ]
    }
   ],
   "source": [
    "bigram_chunker = BigramChunker(train_trees)\n",
    "print(bigram_chunker.evaluate(valid_trees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State short</th>\n",
       "      <th>State full</th>\n",
       "      <th>County</th>\n",
       "      <th>City alias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Holtsville</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>SUFFOLK</td>\n",
       "      <td>Internal Revenue Service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Holtsville</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>SUFFOLK</td>\n",
       "      <td>Holtsville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adjuntas</td>\n",
       "      <td>PR</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>ADJUNTAS</td>\n",
       "      <td>URB San Joaquin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adjuntas</td>\n",
       "      <td>PR</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>ADJUNTAS</td>\n",
       "      <td>Jard De Adjuntas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adjuntas</td>\n",
       "      <td>PR</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>ADJUNTAS</td>\n",
       "      <td>Colinas Del Gigante</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         City State short   State full    County                City alias\n",
       "0  Holtsville          NY     New York   SUFFOLK  Internal Revenue Service\n",
       "1  Holtsville          NY     New York   SUFFOLK                Holtsville\n",
       "2    Adjuntas          PR  Puerto Rico  ADJUNTAS           URB San Joaquin\n",
       "3    Adjuntas          PR  Puerto Rico  ADJUNTAS          Jard De Adjuntas\n",
       "4    Adjuntas          PR  Puerto Rico  ADJUNTAS       Colinas Del Gigante"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_cities = pd.read_csv(\"us_cities_states_counties.csv\", sep=\"|\")\n",
    "us_cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = set(us_cities['City'].str.lower())\n",
    "states = set(us_cities['State full'].str.lower())\n",
    "counties = set(us_cities['County'].str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18854\n",
      "62\n",
      "1932\n"
     ]
    }
   ],
   "source": [
    "print(len(cities))\n",
    "print(len(states))\n",
    "print(len(counties))\n",
    "\n",
    "def gazetteer_lookup(word):\n",
    "    return (word in cities, word in states, word in counties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, True, True)\n",
      "(False, True, True)\n",
      "(True, False, True)\n"
     ]
    }
   ],
   "source": [
    "print(gazetteer_lookup('washington'))\n",
    "print(gazetteer_lookup('utah'))\n",
    "print(gazetteer_lookup('philadelphia'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    \n",
    "    # the first word has both previous word and previous tag undefined\n",
    "    if i == 0:\n",
    "        prevword, prevpos = \"<START>\", \"<START>\"\n",
    "    else:\n",
    "        prevword, prevpos = sentence[i-1]\n",
    "\n",
    "    # gazetteer lookup features (see section below)\n",
    "    gazetteer = gazetteer_lookup(word)\n",
    "\n",
    "    return {\"pos\": pos, \"prevpos\": prevpos, 'word':word,\n",
    "           'word_is_city': gazetteer[0],\n",
    "           'word_is_state': gazetteer[1],\n",
    "           'word_is_county': gazetteer[2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('what', 'WP'),\n",
       " ('flights', 'NNS'),\n",
       " ('leave', 'VBP'),\n",
       " ('atlanta', 'VBN'),\n",
       " ('at', 'IN'),\n",
       " ('about', 'RB'),\n",
       " ('DIGIT', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('afternoon', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('arrive', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('san', 'JJ'),\n",
       " ('francisco', 'NN')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_pos = train_pos[0]\n",
    "sent_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pos': 'WP', 'prevpos': '<START>', 'word': 'what', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'NNS', 'prevpos': 'WP', 'word': 'flights', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'VBP', 'prevpos': 'NNS', 'word': 'leave', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'VBN', 'prevpos': 'VBP', 'word': 'atlanta', 'word_is_city': True, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'IN', 'prevpos': 'VBN', 'word': 'at', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'RB', 'prevpos': 'IN', 'word': 'about', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'NNP', 'prevpos': 'RB', 'word': 'DIGIT', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'IN', 'prevpos': 'NNP', 'word': 'in', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'DT', 'prevpos': 'IN', 'word': 'the', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'NN', 'prevpos': 'DT', 'word': 'afternoon', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'CC', 'prevpos': 'NN', 'word': 'and', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'NN', 'prevpos': 'CC', 'word': 'arrive', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'IN', 'prevpos': 'NN', 'word': 'in', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'JJ', 'prevpos': 'IN', 'word': 'san', 'word_is_city': False, 'word_is_state': False, 'word_is_county': False}\n",
      " \n",
      "{'pos': 'NN', 'prevpos': 'JJ', 'word': 'francisco', 'word_is_city': True, 'word_is_state': False, 'word_is_county': False}\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sent_pos)):\n",
    "    print(npchunk_features(sent_pos, i, history=[]))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsecutiveNPChunkTagger(nltk.TaggerI): \n",
    "\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            # compute features for each word\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = npchunk_features(untagged_sent, i, history) \n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = npchunk_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)\n",
    "\n",
    "class ConsecutiveNPChunker(nltk.ChunkParserI): \n",
    "    def __init__(self, train_sents):\n",
    "        tagged_sents = [[((w,t),c) for (w,t,c) in\n",
    "                         nltk.chunk.tree2conlltags(sent)]\n",
    "                        for sent in train_sents]\n",
    "        self.tagger = ConsecutiveNPChunkTagger(tagged_sents)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        tagged_sents = self.tagger.tag(sentence)\n",
    "        conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = ConsecutiveNPChunker(train_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  91.7%%\n",
      "    Precision:     75.3%%\n",
      "    Recall:        81.8%%\n",
      "    F-Measure:     78.4%%\n"
     ]
    }
   ],
   "source": [
    "print(chunker.evaluate(valid_trees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    \n",
    "    # the first word has both previous word and previous tag undefined\n",
    "    if i == 0:\n",
    "        prevword, prevpos = \"<START>\", \"<START>\"\n",
    "    else:\n",
    "        prevword, prevpos = sentence[i-1]\n",
    "        \n",
    "    if i == len(sentence)-1:\n",
    "        nextword, nextpos = '<END>', '<END>'\n",
    "    else:\n",
    "        nextword, nextpos = sentence[i+1]\n",
    "\n",
    "    # gazetteer lookup features (see section below)\n",
    "    gazetteer = gazetteer_lookup(word)\n",
    "\n",
    "    # adding word_is_digit feature (boolean)\n",
    "    return {\"pos\": pos, \"prevpos\": prevpos, 'word':word, \n",
    "           'word_is_city': gazetteer[0],\n",
    "           'word_is_state': gazetteer[1],\n",
    "           'word_is_county': gazetteer[2],\n",
    "           'word_is_digit': word in 'DIGITDIGITDIGIT', \n",
    "           'nextword': nextword, \n",
    "           'nextpos': nextpos}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  91.7%%\n",
      "    Precision:     75.9%%\n",
      "    Recall:        85.1%%\n",
      "    F-Measure:     80.3%%\n"
     ]
    }
   ],
   "source": [
    "chunker = ConsecutiveNPChunker(train_trees)\n",
    "print(chunker.evaluate(valid_trees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                     pos = 'JJS'          B-cost : O      =  14237.4 : 1.0\n",
      "                    word = 'after'        B-depa : O      =   5457.6 : 1.0\n",
      "           word_is_digit = True           B-depa : O      =   5138.3 : 1.0\n",
      "                    word = 'pm'           I-depa : O      =   4549.1 : 1.0\n",
      "                    word = 'DIGIT'        B-depa : O      =   3148.4 : 1.0\n",
      "                    word = 'expensive'    I-cost : O      =   2635.1 : 1.0\n",
      "                    word = 'DIGITDIGITDIGIT' B-flig : O      =   2320.5 : 1.0\n",
      "                     pos = 'JJR'          B-cost : O      =   2217.6 : 1.0\n",
      "                 prevpos = 'NNP'          I-depa : B-tolo =   1967.9 : 1.0\n",
      "                     pos = 'NNP'          B-depa : O      =   1841.4 : 1.0\n",
      "                    word = 'DIGITDIGITDIGITDIGIT' B-fare : O      =   1840.8 : 1.0\n",
      "                nextword = 'expensive'    B-cost : O      =   1567.3 : 1.0\n",
      "                nextword = 'pm'           B-depa : O      =   1556.6 : 1.0\n",
      "                 prevpos = 'RBS'          I-cost : O      =   1484.6 : 1.0\n",
      "                 prevpos = 'VBZ'          B-airp : B-tolo =   1428.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "chunker.tagger.classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsecutiveNPChunkTagger(nltk.TaggerI): \n",
    "\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            # compute features for each word\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = npchunk_features(untagged_sent, i, history) \n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.DecisionTreeClassifier.train(train_set)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = npchunk_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)\n",
    "\n",
    "class ConsecutiveNPChunker(nltk.ChunkParserI): \n",
    "    def __init__(self, train_sents):\n",
    "        tagged_sents = [[((w,t),c) for (w,t,c) in\n",
    "                         nltk.chunk.tree2conlltags(sent)]\n",
    "                        for sent in train_sents]\n",
    "        self.tagger = ConsecutiveNPChunkTagger(tagged_sents)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        tagged_sents = self.tagger.tag(sentence)\n",
    "        conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  94.8%%\n",
      "    Precision:     83.6%%\n",
      "    Recall:        87.0%%\n",
      "    F-Measure:     85.2%%\n"
     ]
    }
   ],
   "source": [
    "tree_chunker = ConsecutiveNPChunker(train_trees)\n",
    "print(tree_chunker.evaluate(valid_trees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# pip/conda install sklearn_crfsuite\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn_crfsuite import scorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    pos = sent[i][1]\n",
    "    \n",
    "    # first word\n",
    "    if i==0:\n",
    "        prevword = '<START>'\n",
    "        prevpos = '<START>'\n",
    "    else:\n",
    "        prevword = sent[i-1][0]\n",
    "        prevpos = sent[i-1][1]\n",
    "    \n",
    "    # last word\n",
    "    if i == len(sent)-1:\n",
    "        nextword = '<END>'\n",
    "        nextpos = '<END>'\n",
    "    else:\n",
    "        nextword = sent[i+1][0]\n",
    "        nextpos = sent[i+1][1]\n",
    "    \n",
    "    # word is in gazetteer\n",
    "    gazetteer = gazetteer_lookup(word)\n",
    "    \n",
    "    # suffixes and prefixes\n",
    "    pref_1, pref_2, pref_3, pref_4 = word[:1], word[:2], word[:3], word[:4]\n",
    "    suff_1, suff_2, suff_3, suff_4 = word[-1:], word[-2:], word[-3:], word[-4:]\n",
    "    \n",
    "    return {'word':word,\n",
    "            'pos': pos, \n",
    "            'prevword': prevword,\n",
    "            'prevpos': prevpos,  \n",
    "            'nextword': nextword, \n",
    "            'nextpos': nextpos,\n",
    "            'word_is_city': gazetteer[0],\n",
    "            'word_is_state': gazetteer[1],\n",
    "            'word_is_county': gazetteer[2],\n",
    "            'word_is_digit': word in 'DIGITDIGITDIGIT',\n",
    "            'suff_1': suff_1,  \n",
    "            'suff_2': suff_2,  \n",
    "            'suff_3': suff_3,  \n",
    "            'suff_4': suff_4, \n",
    "            'pref_1': pref_1,  \n",
    "            'pref_2': pref_2,  \n",
    "            'pref_3': pref_3, \n",
    "            'pref_4': pref_4 }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'atlanta',\n",
       " 'pos': 'VBN',\n",
       " 'prevword': 'leave',\n",
       " 'prevpos': 'VBP',\n",
       " 'nextword': 'at',\n",
       " 'nextpos': 'IN',\n",
       " 'word_is_city': True,\n",
       " 'word_is_state': False,\n",
       " 'word_is_county': False,\n",
       " 'word_is_digit': False,\n",
       " 'suff_1': 'a',\n",
       " 'suff_2': 'ta',\n",
       " 'suff_3': 'nta',\n",
       " 'suff_4': 'anta',\n",
       " 'pref_1': 'a',\n",
       " 'pref_2': 'at',\n",
       " 'pref_3': 'atl',\n",
       " 'pref_4': 'atla'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_features(train_labels[0], i=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('what', 'WP', 'O'),\n",
       " ('flights', 'NNS', 'O'),\n",
       " ('leave', 'VBP', 'O'),\n",
       " ('atlanta', 'VBN', 'B-fromloc.city_name'),\n",
       " ('at', 'IN', 'O'),\n",
       " ('about', 'RB', 'B-depart_time.time_relative'),\n",
       " ('DIGIT', 'NNP', 'B-depart_time.time'),\n",
       " ('in', 'IN', 'O'),\n",
       " ('the', 'DT', 'O'),\n",
       " ('afternoon', 'NN', 'B-depart_time.period_of_day'),\n",
       " ('and', 'CC', 'O'),\n",
       " ('arrive', 'NN', 'O'),\n",
       " ('in', 'IN', 'O'),\n",
       " ('san', 'JJ', 'B-toloc.city_name'),\n",
       " ('francisco', 'NN', 'I-toloc.city_name')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent):\n",
    "    return [word_features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [sent2features(s) for s in train_labels]\n",
    "y_train = [sent2labels(s) for s in train_labels]\n",
    "\n",
    "X_valid = [sent2features(s) for s in valid_labels]\n",
    "y_valid = [sent2labels(s) for s in valid_labels]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_labels]\n",
    "y_test = [sent2labels(s) for s in test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
