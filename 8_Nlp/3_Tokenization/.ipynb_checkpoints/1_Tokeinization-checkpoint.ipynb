{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At nine o'clock I visted him myself. It looks like religious mania, and he'll seen thinking he himself is God.\n"
     ]
    }
   ],
   "source": [
    "document = \"At nine o'clock I visted him myself. It looks like religious mania, and he'll seen thinking he himself is God.\"\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'nine', \"o'clock\", 'I', 'visted', 'him', 'myself.', 'It', 'looks', 'like', 'religious', 'mania,', 'and', \"he'll\", 'seen', 'thinking', 'he', 'himself', 'is', 'God.']\n"
     ]
    }
   ],
   "source": [
    "print(document.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\avinash.tiwari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "words = word_tokenize(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'nine', \"o'clock\", 'I', 'visted', 'him', 'myself', '.', 'It', 'looks', 'like', 'religious', 'mania', ',', 'and', 'he', \"'ll\", 'seen', 'thinking', 'he', 'himself', 'is', 'God', '.']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentences = sent_tokenize(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"At nine o'clock I visted him myself.\", \"It looks like religious mania, and he'll seen thinking he himself is God.\"]\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitsample = \"WIN with PRIMI to celebrate #NationalPizzaDay! RT this tweet, tell us what your fav PRIMI PIZZA is & stand to WIN a R250 voucher.  ü•≥üçï\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WIN', 'with', 'PRIMI', 'to', 'celebrate', '#', 'NationalPizzaDay', '!', 'RT', 'this', 'tweet', ',', 'tell', 'us', 'what', 'your', 'fav', 'PRIMI', 'PIZZA', 'is', '&', 'stand', 'to', 'WIN', 'a', 'R250', 'voucher', '.', 'ü•≥üçï']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(twitsample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WIN',\n",
       " 'with',\n",
       " 'PRIMI',\n",
       " 'to',\n",
       " 'celebrate',\n",
       " '#NationalPizzaDay',\n",
       " '!',\n",
       " 'RT',\n",
       " 'this',\n",
       " 'tweet',\n",
       " ',',\n",
       " 'tell',\n",
       " 'us',\n",
       " 'what',\n",
       " 'your',\n",
       " 'fav',\n",
       " 'PRIMI',\n",
       " 'PIZZA',\n",
       " 'is',\n",
       " '&',\n",
       " 'stand',\n",
       " 'to',\n",
       " 'WIN',\n",
       " 'a',\n",
       " 'R250',\n",
       " 'voucher',\n",
       " '.',\n",
       " 'ü•≥',\n",
       " 'üçï']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknzr.tokenize(twitsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#NationalPizzaDay']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "pattern = \"#[\\w]+\"\n",
    "print(regexp_tokenize(twitsample,pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
